{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaged-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "front-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run read_c.ipynb\n",
    "%run calc.ipynb\n",
    "%run extractWave.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bca7f6",
   "metadata": {},
   "source": [
    "## 取得有感地震的檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "destroyed-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得某個階段的所有檔名\n",
    "def get_filename_felt(root_dir, year):\n",
    "    dir = os.listdir(root_dir)\n",
    "\n",
    "    allfile = []\n",
    "    for file in dir:\n",
    "        path = os.path.join(root_dir, file)\n",
    "       \n",
    "        tmp = glob.glob(path + '/*.P' + str(year))\n",
    "        tmp1 = glob.glob(path + '/*.[1-9]' + str(year))\n",
    "        allfile = allfile + tmp + tmp1\n",
    "    \n",
    "    return allfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3614f",
   "metadata": {},
   "source": [
    "## 取得無感地震的檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2e9d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_nofelt(root_dir, year):\n",
    "    dir = os.listdir(root_dir)\n",
    "    allfile = []\n",
    "    \n",
    "    for month in dir:\n",
    "        if month == 'felt':\n",
    "            continue\n",
    "            \n",
    "        tmp_path = os.path.join(root_dir, month)\n",
    "        day = os.listdir(tmp_path)\n",
    "        \n",
    "        for d in day:\n",
    "            path = os.path.join(tmp_path, d)\n",
    "            \n",
    "            tmp = glob.glob(path + '/*.P' + str(year))\n",
    "            tmp1 = glob.glob(path + '/*.[1-9]' + str(year))\n",
    "            allfile = allfile + tmp + tmp1\n",
    "            \n",
    "    return allfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663c571",
   "metadata": {},
   "source": [
    "## 以 pfile 為主，把 afile 資訊合併進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "imperial-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_afile_to_pfile(a, p):\n",
    "    tmp_factor = []\n",
    "    \n",
    "    for a_stream in a:\n",
    "        station = a_stream.stats.station\n",
    "        cur_axis, factor, instrument = get_factor(a_stream)\n",
    "        \n",
    "        # 檢查 station 有沒有在 pfile dictionary 裡面出現\n",
    "        if (station not in p.keys()) or (cur_axis == 'none'):\n",
    "            continue\n",
    "        #else:\n",
    "        tmp_factor.append(factor)\n",
    "        \n",
    "        # 先取得要存進 pfile 的 data\n",
    "        network = a_stream.stats.network\n",
    "        location = a_stream.stats.location\n",
    "        sampling_rate = a_stream.stats.sampling_rate\n",
    "        starttime = str(a_stream.stats.starttime)\n",
    "        endtime = str(a_stream.stats.endtime)\n",
    "        channel = a_stream.stats.channel\n",
    "       \n",
    "        # 初始化: 讓 pfile dict 一些欄位轉成 list type\n",
    "        if 'network' not in p[station].keys():\n",
    "            p[station]['network'] = list()\n",
    "        if 'location' not in p[station].keys():\n",
    "            p[station]['location'] = list()\n",
    "        if 'factor' not in p[station].keys():\n",
    "            p[station]['factor'] = list()\n",
    "        if 'sampling_rate' not in p[station].keys():\n",
    "            p[station]['sampling_rate'] = list()\n",
    "        if 'starttime' not in p[station].keys():\n",
    "            p[station]['starttime'] = list()\n",
    "        if 'endtime' not in p[station].keys():\n",
    "            p[station]['endtime'] = list()\n",
    "        if 'instrument' not in p[station].keys():\n",
    "            p[station]['instrument'] = list()\n",
    "        if 'datatype' not in p[station].keys():\n",
    "            p[station]['datatype'] = list()\n",
    "\n",
    "        # 加入 pfile 的 dictionary 之中\n",
    "        if channel == 'Ch3' or channel == 'Ch6' or channel == 'Ch9':\n",
    "            flist = tmp_factor.copy()\n",
    "            p[station]['factor'].append(flist)\n",
    "            p[station]['network'].append(network)\n",
    "            p[station]['location'].append(location)\n",
    "            p[station]['sampling_rate'].append(sampling_rate)\n",
    "            p[station]['starttime'].append(starttime)\n",
    "            p[station]['endtime'].append(endtime)\n",
    "            p[station]['instrument'].append(instrument)\n",
    "            \n",
    "            if channel == 'Ch3':\n",
    "                p[station]['datatype'].append('Acceleration')\n",
    "            else:\n",
    "                p[station]['datatype'].append('Velocity')\n",
    "            \n",
    "            tmp_factor.clear()\n",
    "        \n",
    "        # 加入 E, N, Z 進 dictionary 之中, \n",
    "        if cur_axis == 'z':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'Z' not in p[station].keys():\n",
    "                p[station]['Z'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['Z'] = np.vstack([p[station]['Z'], a_stream.data])\n",
    "        elif cur_axis == 'n':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'N' not in p[station].keys():\n",
    "                p[station]['N'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['N'] = np.vstack([p[station]['N'], a_stream.data])\n",
    "        elif cur_axis == 'e':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'E' not in p[station].keys():\n",
    "                p[station]['E'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['E'] = np.vstack([p[station]['E'], a_stream.data])\n",
    "       \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b65522",
   "metadata": {},
   "source": [
    "## 把波型資料轉成 list，才能存進 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optical-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arr_to_list(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            for sub_key in p[k].keys():\n",
    "                if sub_key == 'E' or sub_key == 'N' or sub_key == 'Z': \n",
    "                    p[k][sub_key] = p[k][sub_key].tolist()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246af68",
   "metadata": {},
   "source": [
    "## 把沒有波型資料的測站刪除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "miniature-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_no_data(p):\n",
    "    del_sta = []\n",
    "    for k in p.keys():\n",
    "            try:\n",
    "                if ('E' not in p[k].keys()) or ('N' not in p[k].keys()) or ('Z' not in p[k].keys()):\n",
    "                    del_sta.append(k)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    for todel in del_sta:\n",
    "        del p[todel]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bab81",
   "metadata": {},
   "source": [
    "## 因為單一測站有多組資料，複製到時、震度、PGA、PGV 數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "300df346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_values(p):\n",
    "    year = int(p['ori_time'][:4])\n",
    "    \n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # 有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "               \n",
    "                # 複製 p & s_arrival time, intensity\n",
    "                p_time = p[k]['p_arrival_time']\n",
    "                s_time = p[k]['s_arrival_time']\n",
    "                S_avail = p[k]['S']\n",
    "                intensity = p[k]['intensity']\n",
    "                pga = p[k]['pga']\n",
    "\n",
    "                # ============================================= #\n",
    "                #     舊制的 intensirty, pga, pgv 都為 False     #\n",
    "                # ============================================= #\n",
    "                # check intensity, pga, pgv\n",
    "                is_intensity = False\n",
    "                is_pga = False\n",
    "                is_pgv = False\n",
    "                \n",
    "                # ============================================= #\n",
    "                #           檢查 intensity, pga, pgv            #\n",
    "                # ============================================= #\n",
    "                # 只有 2020 之後的有 pgv\n",
    "                if year >= 2020:\n",
    "                    pgv = p[k]['pgv']\n",
    "                    del p[k]['pgv']\n",
    "                    \n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    if intensity == -1:\n",
    "                        is_intensity = False\n",
    "                    else:\n",
    "                        is_intensity = True\n",
    "                    if pga == -1 or pga == 0:\n",
    "                        is_pga = False\n",
    "                    else:\n",
    "                        is_pga = True\n",
    "                    if pgv == -1 or pgv == 0:\n",
    "                        is_pgv = False\n",
    "                    else:\n",
    "                        is_pgv = True\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(pgv)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                # 2019 以前都沒有 PGV，先用 nan 代替\n",
    "                else:\n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(-1)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                        \n",
    "                # ============================================= #\n",
    "                #          刪除原始欄位，改用 list 取代           #\n",
    "                # ============================================= #\n",
    "                del p[k]['p_arrival_time']\n",
    "                del p[k]['s_arrival_time']\n",
    "                del p[k]['intensity']\n",
    "                del p[k]['pga']\n",
    "                del p[k]['S']\n",
    "                \n",
    "                p[k]['p_arrival_time'] = []\n",
    "                p[k]['s_arrival_time'] = []\n",
    "                p[k]['intensity'] = []\n",
    "                p[k]['instrument_isWork'] = []\n",
    "                p[k]['pga'] = []\n",
    "                p[k]['isIntensity'] = []\n",
    "                p[k]['isPga'] = []\n",
    "                p[k]['isStime'] = []\n",
    "                \n",
    "                # ============================================= #\n",
    "                #       複製原始資料裡面的一些 attributes         #\n",
    "                # ============================================= #\n",
    "                for i in range(n_data):\n",
    "                    p[k]['p_arrival_time'].append(p_time)\n",
    "                    p[k]['s_arrival_time'].append(s_time)\n",
    "                    p[k]['intensity'].append(intensity)\n",
    "                    p[k]['instrument_isWork'].append(True)\n",
    "                    p[k]['pga'].append(pga)\n",
    "                    p[k]['isIntensity'].append(is_intensity)\n",
    "                    p[k]['isPga'].append(is_pga)\n",
    "                    p[k]['isStime'].append(S_avail)\n",
    "                \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025b371",
   "metadata": {},
   "source": [
    "## 整合多項數據的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bb06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_attributes(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # ============================================= #\n",
    "                #              取得數據的有效性 list             #\n",
    "                # ============================================= #\n",
    "                instrument = p[k]['instrument_isWork']\n",
    "                intensity = p[k]['isIntensity']\n",
    "                pga = p[k]['isPga']\n",
    "                pgv = p[k]['isPgv']\n",
    "                s = p[k]['isStime']\n",
    "               \n",
    "                del p[k]['instrument_isWork']\n",
    "                del p[k]['isIntensity']\n",
    "                del p[k]['isPga']\n",
    "                del p[k]['isPgv']\n",
    "                del p[k]['isStime']\n",
    "                \n",
    "                avail = {}\n",
    "                avail['instrument'] = instrument\n",
    "                avail['intensity'] = intensity\n",
    "                avail['pga'] = pga\n",
    "                avail['pgv'] = pgv\n",
    "                avail['Stime'] = s\n",
    "                p[k]['DataAvailable'] = avail\n",
    "        except:\n",
    "            pass\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eab55",
   "metadata": {},
   "source": [
    "## 把同測站不同組數據整合在同個 key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7cf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_json(p, files, year):\n",
    "    # 新增 primary key 欄位\n",
    "    month = int(files[:2]) - 12\n",
    "    month = str(month) if month//10==1 else '0'+str(month)\n",
    "    version = files[-7]\n",
    "    p['event'] = year[-2:] + str(month) + files[2:8] + version\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            if 'location' in p[k].keys():\n",
    "                # 看有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "                output_dict = {}\n",
    "                p[k]['numberOfData'] = n_data\n",
    "\n",
    "                # 逐一拿出資料\n",
    "                for n in range(n_data):\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict['network'] = p[k]['network'][n]\n",
    "                    tmp_dict['location'] = p[k]['location'][n]\n",
    "                    tmp_dict['factor'] = p[k]['factor'][n]\n",
    "                    tmp_dict['sampling_rate'] = p[k]['sampling_rate'][n]\n",
    "                    tmp_dict['starttime'] = p[k]['starttime'][n]\n",
    "                    tmp_dict['endtime'] = p[k]['endtime'][n]\n",
    "                    tmp_dict['instrument'] = p[k]['instrument'][n]\n",
    "                    tmp_dict['datatype'] = p[k]['datatype'][n]\n",
    "                    if n_data > 1:\n",
    "                        tmp_dict['Z'], tmp_dict['N'], tmp_dict['E'] = p[k]['Z'][n], p[k]['N'][n], p[k]['E'][n]\n",
    "                    else:\n",
    "                        tmp_dict['Z'], tmp_dict['N'], tmp_dict['E'] = p[k]['Z'], p[k]['N'], p[k]['E']\n",
    "                    tmp_dict['pga'], tmp_dict['pgv'] = p[k]['pga'][n], p[k]['pgv'][n]\n",
    "                    tmp_dict['p_arrival_time'], tmp_dict['s_arrival_time'] = p[k]['p_arrival_time'][n], p[k]['s_arrival_time'][n]\n",
    "                    tmp_dict['intensity'] = p[k]['intensity'][n]\n",
    "                    tmp_dict['DataAvailable'] = {}\n",
    "                    for key in p[k]['DataAvailable'].keys():\n",
    "                        tmp_dict['DataAvailable'][key] = p[k]['DataAvailable'][key][n]\n",
    "\n",
    "                    # 新增到依順序建立的新 key\n",
    "                    output_dict[str(n)] = tmp_dict\n",
    "\n",
    "                # 刪除要改掉的 keys\n",
    "                del p[k]['network'], p[k]['location'], p[k]['factor'], p[k]['sampling_rate'], p[k]['starttime']\n",
    "                del p[k]['endtime'], p[k]['instrument'], p[k]['datatype'], p[k]['Z'], p[k]['N'], p[k]['E']\n",
    "                del p[k]['pga'], p[k]['pgv'], p[k]['p_arrival_time'], p[k]['s_arrival_time']\n",
    "                del p[k]['intensity'], p[k]['DataAvailable']\n",
    "\n",
    "                # 把改好的加進原始資料中\n",
    "                for modify_k in output_dict.keys():\n",
    "                    p[k][modify_k] = output_dict[modify_k]\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-withdrawal",
   "metadata": {},
   "source": [
    "# 從這裡開始執行以產生 json 檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-vision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory... /mnt/nas2/CWBSN_nofelt/2018\n",
      "making directory... /mnt/nas2/CWBSN_nofelt/2018/wave\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                         | 126/31555 [21:13<81:49:48,  9.37s/it]"
     ]
    }
   ],
   "source": [
    "# finish: 2012, \n",
    "#all_year = ['2020', '2019', '2018', '2017', '2016', '2015', '2014']\n",
    "all_year = ['2018', '2019', '2020', '2021']\n",
    "for year in all_year:\n",
    "    sub_fname = year[2:]\n",
    "    base_path = '/mnt/nas8/CWBSN/' + year\n",
    "    save_base_path = os.path.join('/mnt/nas2/CWBSN_nofelt', year)\n",
    "    wave_save_path = os.path.join(save_base_path, 'wave')\n",
    "    files = get_filename_nofelt(base_path, year[-2:])\n",
    "\n",
    "    # mkdir\n",
    "    if not os.path.exists(save_base_path):\n",
    "        print('making directory...', save_base_path)\n",
    "        os.mkdir(save_base_path)\n",
    "    if not os.path.exists(wave_save_path):\n",
    "        print('making directory...', wave_save_path)\n",
    "        os.mkdir(wave_save_path)\n",
    "        \n",
    "    # generate dataset\n",
    "    gen(files, save_base_path, sub_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "willing-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(files, save_base_path, sub_fname):\n",
    "    for f in tqdm(range(len(files))):\n",
    "        try:\n",
    "            filename = files[f][:-4]\n",
    "            pfile = files[f]\n",
    "            num_p = files[f][-3]\n",
    "            if num_p == 'P':\n",
    "                num_p = '0'\n",
    "\n",
    "            # output json file\n",
    "            json_file = files[f][-12:-4] + '(' + num_p + ')' + '.json'\n",
    "            save_path = os.path.join(save_base_path, json_file)\n",
    "\n",
    "            # if repeated, don't save as json\n",
    "            #if os.path.exists(save_path):\n",
    "                #continue\n",
    "\n",
    "            a = unpackAfile(filename + '.A' + sub_fname)\n",
    "            if sub_fname == '20' or sub_fname == '21':\n",
    "                p = unpackPfile_2020(pfile)  # 2020 ~\n",
    "            else:\n",
    "                p = unpackPfile(pfile)   # ~ 2019 \n",
    "\n",
    "            # 把 afile 資訊加入 pfile's dictionary\n",
    "            p = append_afile_to_pfile(a, p)\n",
    "\n",
    "            # 把 pfile 裡面的 ndarray 轉換成 list 才能存進 dictionary\n",
    "            p = convert_arr_to_list(p)    \n",
    "\n",
    "            # 把沒有加速度資料的測站刪掉\n",
    "            p = delete_no_data(p)\n",
    "\n",
    "            # 改一些欄位\n",
    "            p = copy_values(p)\n",
    "\n",
    "            # 整合一些欄位\n",
    "            p = concat_attributes(p)\n",
    "\n",
    "            # 最後修改 json 欄位\n",
    "            p = modify_json(p, json_file, sub_fname)\n",
    "\n",
    "            # 新制震度\n",
    "            p = modify(p)\n",
    "            \n",
    "            # 波型取出來另外存\n",
    "            p = extractWave(p, wave_save_path)\n",
    "            \n",
    "            # write\n",
    "            with open(save_path, 'w') as file:\n",
    "                json.dump(p, file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e212fd",
   "metadata": {},
   "source": [
    "## 重跑遺失的單一事件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d045b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"/mnt/nas6/origin_CWB_data/CWB_felt/2021/felt/01/13010457.P21\"\n",
    "save_base_path = os.path.join('/mnt/nas6/CWBSN', '2017')\n",
    "wave_save_path = '/code/blog/static/blog/images/CWBSN/' + year + '/wave'\n",
    "sub_fname='21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fec6767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'str' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'int' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'float' object has no attribute 'keys'\n",
      "'str' object has no attribute 'keys'\n",
      "'int' object has no attribute 'keys'\n",
      "'str' object has no attribute 'keys'\n",
      "2\n",
      "2021-01-01 04:57:47.920000 2021-01-01 04:57:50.390000 True 1 2.3\n",
      "3\n",
      "2021-01-01 04:57:48.280000 2021-01-01 04:57:50.930000 True 1 1.3\n",
      "4\n",
      "2021-01-01 04:57:49.140000 2021-01-01 04:57:52.450000 True 0 0.7\n",
      "2\n",
      "2021-01-01 04:57:49.410000 2021-01-01 04:57:53.250000 True 1 1.4\n",
      "3\n",
      "2021-01-01 04:57:49.430000 2021-01-01 04:57:52.920000 True 0 0.4\n",
      "3\n",
      "2021-01-01 04:57:49.730000 2021-01-01 04:57:53.760000 True 1 1.8\n",
      "1\n",
      "2021-01-01 04:57:49.980000 2021-01-01 04:57:54.480000 True 2 3.5\n",
      "1\n",
      "2021-01-01 04:57:50.050000 2021-01-01 04:57:54.040000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:50.630000 2021-01-01 04:57:55 True 0 0.5\n",
      "3\n",
      "2021-01-01 04:57:50.460000 2021-01-01 04:57:54.980000 True 0 0.7\n",
      "1\n",
      "2021-01-01 04:57:51.170000 2021-01-01 04:57:55.800000 True 0 0.3\n",
      "2\n",
      "2021-01-01 04:57:51.110000 2021-01-01 04:57:56.130000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:51.310000 2021-01-01 04:57:56.400000 True 0 0.2\n",
      "2\n",
      "2021-01-01 04:57:51.090000 2021-01-01 04:57:56.210000 True -1 0.0\n",
      "4\n",
      "2021-01-01 04:57:52.210000 2021-01-01 04:57:58.210000 True 0 0.3\n",
      "2\n",
      "2021-01-01 04:57:51.950000 2021-01-01 04:57:57.760000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:52.260000 2021-01-01 04:57:58.230000 True 0 0.4\n",
      "1\n",
      "2021-01-01 04:57:52.600000 2021-01-01 04:57:58.600000 True 0 0.3\n",
      "1\n",
      "2021-01-01 04:57:52.400000 2021-01-01 04:57:58.430000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:53.050000 2021-01-01 04:57:59.040000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:53.540000 2021-01-01 04:58:00.160000 True 0 0.4\n",
      "1\n",
      "2021-01-01 04:57:53.130000 2021-01-01 04:57:59.150000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:53.470000 2021-01-01 04:57:59.200000 True 0 0.1\n",
      "2\n",
      "2021-01-01 04:57:53.370000 2021-01-01 04:58:00.220000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:53.550000 2021-01-01 04:58:00.630000 True 1 1.5\n",
      "1\n",
      "2021-01-01 04:57:53.460000 2021-01-01 04:58:00.600000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:53.740000 2021-01-01 04:58:00.400000 True 0 0.2\n",
      "2\n",
      "2021-01-01 04:57:54.020000 2021-01-01 04:58:01.020000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:54.620000 2021-01-01 04:58:02.050000 True 2 3.5\n",
      "1\n",
      "2021-01-01 04:57:54.610000 2021-01-01 04:58:01.820000 True 1 1.8\n",
      "2\n",
      "2021-01-01 04:57:54.110000 2021-01-01 04:58:01.780000 True 0 0.5\n",
      "2\n",
      "2021-01-01 04:57:54.330000 2021-01-01 04:58:02.040000 True 0 0.2\n",
      "3\n",
      "2021-01-01 04:57:54.180000 2021-01-01 04:58:01.600000 True 0 0.1\n",
      "1\n",
      "2021-01-01 04:57:54.470000 2021-01-01 04:58:01.210000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:54.890000 2021-01-01 04:58:03.250000 True 0 0.7\n",
      "3\n",
      "2021-01-01 04:57:54.930000 2021-01-01 04:58:01.670000 True 0 0.1\n",
      "2\n",
      "2021-01-01 04:57:55.260000 2021-01-01 04:58:02.700000 True 0 0.2\n",
      "2\n",
      "2021-01-01 04:57:54.920000 2021-01-01 04:58:02.680000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:55.480000 2021-01-01 04:58:04.150000 True 0 0.2\n",
      "7\n",
      "2021-01-01 04:57:55.370000 2021-01-01 04:58:02.710000 True 0 0.1\n",
      "2\n",
      "2021-01-01 04:57:56.170000 2021-01-01 04:58:04.160000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:56.240000 2021-01-01 04:58:04.260000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:56.510000 2021-01-01 04:58:04.730000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:56.440000 2021-01-01 04:58:05.630000 True 0 0.2\n",
      "2\n",
      "2021-01-01 04:57:55.510000 2021-01-01 04:58:04.460000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:56.780000 2021-01-01 04:58:04.560000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:56.820000 2021-01-01 04:58:05.180000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:56.830000 2021-01-01 04:58:05.510000 True 0 0.1\n",
      "1\n",
      "2021-01-01 04:57:58.080000 2021-01-01 04:57:00 False -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:57.870000 2021-01-01 04:58:06.670000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:57.930000 2021-01-01 04:58:07.610000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:57.990000 2021-01-01 04:58:06.700000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:57.950000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:58.840000 2021-01-01 04:58:08.830000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:58.540000 2021-01-01 04:58:08 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:58.680000 2021-01-01 04:58:09.870000 True 0 0.3\n",
      "2\n",
      "2021-01-01 04:57:58.890000 2021-01-01 04:58:09.050000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:59.210000 2021-01-01 04:58:09.140000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:59.080000 2021-01-01 04:58:09.810000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:57:58.450000 2021-01-01 04:58:08.280000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:58.270000 2021-01-01 04:58:07.780000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:58.910000 2021-01-01 04:58:08.450000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:00.420000 2021-01-01 04:57:00 False -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:59.460000 2021-01-01 04:58:11.490000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:58.510000 2021-01-01 04:58:10.530000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:57:59.360000 2021-01-01 04:58:11.160000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:57:59.990000 2021-01-01 04:58:10.950000 True 0 0.4\n",
      "3\n",
      "2021-01-01 04:57:59.250000 2021-01-01 04:58:11.440000 True 0 0.7\n",
      "2\n",
      "2021-01-01 04:58:01.320000 2021-01-01 04:58:13.670000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:01.980000 2021-01-01 04:58:15.260000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:01.390000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:03.400000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:01.930000 2021-01-01 04:58:14.390000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:01.720000 2021-01-01 04:57:00 False -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:03.500000 2021-01-01 04:58:15 True 0 0.1\n",
      "2\n",
      "2021-01-01 04:58:01.910000 2021-01-01 04:58:13.650000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:02.280000 2021-01-01 04:58:15.120000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:02.590000 2021-01-01 04:58:15.140000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:03.270000 2021-01-01 04:58:18.340000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:02.970000 2021-01-01 04:58:15.850000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:03.930000 2021-01-01 04:58:17.440000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:04.480000 2021-01-01 04:58:18.650000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:04.250000 2021-01-01 04:58:17.810000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:04.540000 2021-01-01 04:58:19.570000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:04.920000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:04.210000 2021-01-01 04:58:18.420000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:04.040000 2021-01-01 04:58:18.120000 True -1 0.0\n",
      "4\n",
      "2021-01-01 04:58:04.540000 2021-01-01 04:58:18.640000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:06.100000 2021-01-01 04:58:20.610000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:04.890000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:06.480000 2021-01-01 04:58:21.590000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:07.550000 2021-01-01 04:58:24.370000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:07.760000 2021-01-01 04:57:00 False -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:08.090000 2021-01-01 04:58:25.630000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:08.290000 2021-01-01 04:58:25.740000 True -1 0.0\n",
      "4\n",
      "2021-01-01 04:58:08.290000 2021-01-01 04:58:26.910000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:06.480000 2021-01-01 04:58:24.220000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:09.040000 2021-01-01 04:58:26.910000 True -1 0.0\n",
      "3\n",
      "2021-01-01 04:58:09.580000 2021-01-01 04:58:27.750000 True -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:09.680000 2021-01-01 04:58:27.970000 True -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:11.040000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:10.630000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:13.720000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:13.400000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:15.100000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:18.400000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:15.850000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:13.930000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:19.890000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:21.910000 2021-01-01 04:57:00 False -1 0.0\n",
      "2\n",
      "2021-01-01 04:58:22.760000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:24.570000 2021-01-01 04:57:00 False -1 0.0\n",
      "1\n",
      "2021-01-01 04:58:27.970000 2021-01-01 04:57:00 False -1 0.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/code/blog/static/blog/images/CWBSN/2021/wave/21010104570'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m p \u001b[38;5;241m=\u001b[39m modify_json(p, json_file, sub_fname)\n\u001b[1;32m     32\u001b[0m p \u001b[38;5;241m=\u001b[39m modify(p)\n\u001b[0;32m---> 34\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mextractWave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwave_save_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_22842/421182889.py:5\u001b[0m, in \u001b[0;36mextractWave\u001b[0;34m(p, save_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print('creating directiory: %s' %(save_path))\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# 看測站內有多少組波形資料\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/code/blog/static/blog/images/CWBSN/2021/wave/21010104570'"
     ]
    }
   ],
   "source": [
    "filename = files[:-4]\n",
    "pfile = files\n",
    "num_p = files[-3]\n",
    "if num_p == 'P':\n",
    "    num_p = '0'\n",
    "\n",
    "# output json file\n",
    "save_path = os.path.join(save_base_path, files[-12:-4]) + '(' + num_p + ')' + '.json'\n",
    "\n",
    "a = unpackAfile(filename + '.A' + sub_fname)\n",
    "#p = unpackPfile(pfile)   # ~ 2019 \n",
    "p = unpackPfile_2020(pfile)  # 2020 ~\n",
    "\n",
    "# 把 afile 資訊加入 pfile's dictionary\n",
    "p = append_afile_to_pfile(a, p)\n",
    "# 把 pfile 裡面的 ndarray 轉換成 list 才能存進 dictionary\n",
    "p = convert_arr_to_list(p)    \n",
    "\n",
    "# 把沒有加速度資料的測站刪掉\n",
    "p = delete_no_data(p)\n",
    "\n",
    "# 改一些欄位\n",
    "p = copy_values(p)\n",
    "\n",
    "# 整合一些欄位\n",
    "p = concat_attributes(p)\n",
    "\n",
    "# 最後修改 json 欄位\n",
    "json_file = files[-12:-4] + '(' + num_p + ')' + '.json'\n",
    "p = modify_json(p, json_file, sub_fname)\n",
    "\n",
    "p = modify(p)\n",
    "\n",
    "p = extractWave(p, wave_save_path)\n",
    "# write\n",
    "#with open(save_path, 'w') as file:\n",
    "    #json.dump(p, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344bc43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthquake",
   "language": "python",
   "name": "earthquake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
