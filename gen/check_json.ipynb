{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846ae636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "aec13a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run read_c.ipynb\n",
    "%run calc.ipynb\n",
    "%run extractWave.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "654c9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_afile_to_pfile(a, p):\n",
    "    tmp_factor = []\n",
    "    \n",
    "    for a_stream in a:\n",
    "        station = a_stream.stats.station\n",
    "        cur_axis, factor, instrument = get_factor(a_stream)\n",
    "        \n",
    "        # 檢查 station 有沒有在 pfile dictionary 裡面出現\n",
    "        if (station not in p.keys()) or (cur_axis == 'none'):\n",
    "            continue\n",
    "        #else:\n",
    "        tmp_factor.append(factor)\n",
    "        \n",
    "        # 先取得要存進 pfile 的 data\n",
    "        network = a_stream.stats.network\n",
    "        location = a_stream.stats.location\n",
    "        sampling_rate = a_stream.stats.sampling_rate\n",
    "        starttime = str(a_stream.stats.starttime)\n",
    "        endtime = str(a_stream.stats.endtime)\n",
    "        channel = a_stream.stats.channel\n",
    "       \n",
    "        # 初始化: 讓 pfile dict 一些欄位轉成 list type\n",
    "        if 'network' not in p[station].keys():\n",
    "            p[station]['network'] = list()\n",
    "        if 'location' not in p[station].keys():\n",
    "            p[station]['location'] = list()\n",
    "        if 'factor' not in p[station].keys():\n",
    "            p[station]['factor'] = list()\n",
    "        if 'sampling_rate' not in p[station].keys():\n",
    "            p[station]['sampling_rate'] = list()\n",
    "        if 'starttime' not in p[station].keys():\n",
    "            p[station]['starttime'] = list()\n",
    "        if 'endtime' not in p[station].keys():\n",
    "            p[station]['endtime'] = list()\n",
    "        if 'instrument' not in p[station].keys():\n",
    "            p[station]['instrument'] = list()\n",
    "        if 'datatype' not in p[station].keys():\n",
    "            p[station]['datatype'] = list()\n",
    "\n",
    "        # 加入 pfile 的 dictionary 之中\n",
    "        if channel == 'Ch3' or channel == 'Ch6' or channel == 'Ch9':\n",
    "            flist = tmp_factor.copy()\n",
    "            p[station]['factor'].append(flist)\n",
    "            p[station]['network'].append(network)\n",
    "            p[station]['location'].append(location)\n",
    "            p[station]['sampling_rate'].append(sampling_rate)\n",
    "            p[station]['starttime'].append(starttime)\n",
    "            p[station]['endtime'].append(endtime)\n",
    "            p[station]['instrument'].append(instrument)\n",
    "            \n",
    "            if channel == 'Ch3':\n",
    "                p[station]['datatype'].append('Acceleration')\n",
    "            else:\n",
    "                p[station]['datatype'].append('Velocity')\n",
    "            \n",
    "            tmp_factor.clear()\n",
    "        \n",
    "        # 加入 E, N, Z 進 dictionary 之中, \n",
    "        if cur_axis == 'z':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'Z' not in p[station].keys():\n",
    "                p[station]['Z'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['Z'] = np.vstack([p[station]['Z'], a_stream.data])\n",
    "        elif cur_axis == 'n':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'N' not in p[station].keys():\n",
    "                p[station]['N'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['N'] = np.vstack([p[station]['N'], a_stream.data])\n",
    "        elif cur_axis == 'e':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'E' not in p[station].keys():\n",
    "                p[station]['E'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['E'] = np.vstack([p[station]['E'], a_stream.data])\n",
    "       \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "93de7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arr_to_list(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            for sub_key in p[k].keys():\n",
    "                if sub_key == 'E' or sub_key == 'N' or sub_key == 'Z': \n",
    "                    p[k][sub_key] = p[k][sub_key].tolist()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "29760d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_no_data(p):\n",
    "    del_sta = []\n",
    "    for k in p.keys():\n",
    "            try:\n",
    "                if ('E' not in p[k].keys()) or ('N' not in p[k].keys()) or ('Z' not in p[k].keys()):\n",
    "                    del_sta.append(k)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    for todel in del_sta:\n",
    "        del p[todel]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f31dd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_values(p):\n",
    "    year = int(p['ori_time'][:4])\n",
    "    \n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # 有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "               \n",
    "                # 複製 p & s_arrival time, intensity\n",
    "                p_time = p[k]['p_arrival_time']\n",
    "                s_time = p[k]['s_arrival_time']\n",
    "                S_avail = p[k]['S']\n",
    "                intensity = p[k]['intensity']\n",
    "                pga = p[k]['pga']\n",
    "\n",
    "                # ============================================= #\n",
    "                #     舊制的 intensirty, pga, pgv 都為 False     #\n",
    "                # ============================================= #\n",
    "                # check intensity, pga, pgv\n",
    "                is_intensity = False\n",
    "                is_pga = False\n",
    "                is_pgv = False\n",
    "                \n",
    "                # ============================================= #\n",
    "                #           檢查 intensity, pga, pgv            #\n",
    "                # ============================================= #\n",
    "                # 只有 2020 之後的有 pgv\n",
    "                if year >= 2020:\n",
    "                    pgv = p[k]['pgv']\n",
    "                    del p[k]['pgv']\n",
    "                    \n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    if intensity == -1:\n",
    "                        is_intensity = False\n",
    "                    else:\n",
    "                        is_intensity = True\n",
    "                    if pga == -1 or pga == 0:\n",
    "                        is_pga = False\n",
    "                    else:\n",
    "                        is_pga = True\n",
    "                    if pgv == -1 or pgv == 0:\n",
    "                        is_pgv = False\n",
    "                    else:\n",
    "                        is_pgv = True\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(pgv)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                # 2019 以前都沒有 PGV，先用 nan 代替\n",
    "                else:\n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(-1)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                        \n",
    "                # ============================================= #\n",
    "                #          刪除原始欄位，改用 list 取代           #\n",
    "                # ============================================= #\n",
    "                del p[k]['p_arrival_time']\n",
    "                del p[k]['s_arrival_time']\n",
    "                del p[k]['intensity']\n",
    "                del p[k]['pga']\n",
    "                del p[k]['S']\n",
    "                \n",
    "                p[k]['p_arrival_time'] = []\n",
    "                p[k]['s_arrival_time'] = []\n",
    "                p[k]['intensity'] = []\n",
    "                p[k]['instrument_isWork'] = []\n",
    "                p[k]['pga'] = []\n",
    "                p[k]['isIntensity'] = []\n",
    "                p[k]['isPga'] = []\n",
    "                p[k]['isStime'] = []\n",
    "                \n",
    "                # ============================================= #\n",
    "                #       複製原始資料裡面的一些 attributes         #\n",
    "                # ============================================= #\n",
    "                for i in range(n_data):\n",
    "                    p[k]['p_arrival_time'].append(p_time)\n",
    "                    p[k]['s_arrival_time'].append(s_time)\n",
    "                    p[k]['intensity'].append(intensity)\n",
    "                    p[k]['instrument_isWork'].append(True)\n",
    "                    p[k]['pga'].append(pga)\n",
    "                    p[k]['isIntensity'].append(is_intensity)\n",
    "                    p[k]['isPga'].append(is_pga)\n",
    "                    p[k]['isStime'].append(S_avail)\n",
    "                \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2e8ba79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_attributes(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # ============================================= #\n",
    "                #              取得數據的有效性 list             #\n",
    "                # ============================================= #\n",
    "                instrument = p[k]['instrument_isWork']\n",
    "                intensity = p[k]['isIntensity']\n",
    "                pga = p[k]['isPga']\n",
    "                pgv = p[k]['isPgv']\n",
    "                s = p[k]['isStime']\n",
    "               \n",
    "                del p[k]['instrument_isWork']\n",
    "                del p[k]['isIntensity']\n",
    "                del p[k]['isPga']\n",
    "                del p[k]['isPgv']\n",
    "                del p[k]['isStime']\n",
    "                \n",
    "                avail = {}\n",
    "                avail['instrument'] = instrument\n",
    "                avail['intensity'] = intensity\n",
    "                avail['pga'] = pga\n",
    "                avail['pgv'] = pgv\n",
    "                avail['Stime'] = s\n",
    "                p[k]['DataAvailable'] = avail\n",
    "        except:\n",
    "            pass\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3831911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_json(p, files, year):\n",
    "    # 新增 primary key 欄位\n",
    "    month = int(files[:2]) - 12\n",
    "    month = str(month) if month//10==1 else '0'+str(month)\n",
    "    version = files[-7]\n",
    "    p['event'] = year[-2:] + str(month) + files[2:8] + version\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            if 'location' in p[k].keys():\n",
    "                # 看有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "                output_dict = {}\n",
    "                p[k]['numberOfData'] = n_data\n",
    "\n",
    "                # 逐一拿出資料\n",
    "                for n in range(n_data):\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict['network'] = p[k]['network'][n]\n",
    "                    tmp_dict['location'] = p[k]['location'][n]\n",
    "                    tmp_dict['factor'] = p[k]['factor'][n]\n",
    "                    tmp_dict['sampling_rate'] = p[k]['sampling_rate'][n]\n",
    "                    tmp_dict['starttime'] = p[k]['starttime'][n]\n",
    "                    tmp_dict['endtime'] = p[k]['endtime'][n]\n",
    "                    tmp_dict['instrument'] = p[k]['instrument'][n]\n",
    "                    tmp_dict['datatype'] = p[k]['datatype'][n]\n",
    "                    if n_data > 1:\n",
    "                        tmp_dict['Z'], tmp_dict['N'], tmp_dict['E'] = p[k]['Z'][n], p[k]['N'][n], p[k]['E'][n]\n",
    "                    else:\n",
    "                        tmp_dict['Z'], tmp_dict['N'], tmp_dict['E'] = p[k]['Z'], p[k]['N'], p[k]['E']\n",
    "                    tmp_dict['pga'], tmp_dict['pgv'] = p[k]['pga'][n], p[k]['pgv'][n]\n",
    "                    tmp_dict['p_arrival_time'], tmp_dict['s_arrival_time'] = p[k]['p_arrival_time'][n], p[k]['s_arrival_time'][n]\n",
    "                    tmp_dict['intensity'] = p[k]['intensity'][n]\n",
    "                    tmp_dict['DataAvailable'] = {}\n",
    "                    for key in p[k]['DataAvailable'].keys():\n",
    "                        tmp_dict['DataAvailable'][key] = p[k]['DataAvailable'][key][n]\n",
    "\n",
    "                    # 新增到依順序建立的新 key\n",
    "                    output_dict[str(n)] = tmp_dict\n",
    "\n",
    "                # 刪除要改掉的 keys\n",
    "                del p[k]['network'], p[k]['location'], p[k]['factor'], p[k]['sampling_rate'], p[k]['starttime']\n",
    "                del p[k]['endtime'], p[k]['instrument'], p[k]['datatype'], p[k]['Z'], p[k]['N'], p[k]['E']\n",
    "                del p[k]['pga'], p[k]['pgv'], p[k]['p_arrival_time'], p[k]['s_arrival_time']\n",
    "                del p[k]['intensity'], p[k]['DataAvailable']\n",
    "\n",
    "                # 把改好的加進原始資料中\n",
    "                for modify_k in output_dict.keys():\n",
    "                    p[k][modify_k] = output_dict[modify_k]\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9e37c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed(path, base_path):\n",
    "    file = os.listdir(path)\n",
    "    year = path[-4:]\n",
    "    failed = []\n",
    "    \n",
    "    for f in file:\n",
    "        if f == 'wave':\n",
    "            continue\n",
    "\n",
    "        # 超過 1MB\n",
    "        if os.path.getsize(os.path.join(path, f)) >= 1000000:\n",
    "            failed.append(f)\n",
    "    \n",
    "    # 13032259(1).json => 13032259.119\n",
    "    for idx, f in enumerate(failed):\n",
    "        if f[9] == '0':\n",
    "            tmp = f[:8] + '.P' + year[2:]\n",
    "        else:\n",
    "            tmp = f[:8] + '.' + f[9] + year[2:]\n",
    "            \n",
    "        month = int(f[:2]) - 12\n",
    "        month = str(month) if month//10==1 else '0'+str(month)\n",
    "        root_path = os.path.join(base_path, month)\n",
    "        failed[idx] = os.path.join(root_path, tmp)    \n",
    "        \n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "86cb9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wave(wave_save_path, event):\n",
    "    toRemove = os.path.join(wave_save_path, event)\n",
    "    files = os.listdir(toRemove)\n",
    "    \n",
    "    for f in files:\n",
    "        os.remove(os.path.join(toRemove, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "811c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(files, base_path, wave_save_path, sub_fname, mode):\n",
    "    for f in tqdm(range(len(files))):\n",
    "        try:\n",
    "            filename = files[f][:-4]\n",
    "            pfile = files[f]\n",
    "            num_p = files[f][-3]\n",
    "            if num_p == 'P':\n",
    "                num_p = '0'\n",
    "\n",
    "            # output json file\n",
    "            json_file = files[f][-12:-4] + '(' + num_p + ')' + '.json'\n",
    "            save_path = os.path.join(save_base_path, json_file)\n",
    "\n",
    "            # if repeated, don't save as json\n",
    "            #if os.path.exists(save_path):\n",
    "                #continue\n",
    "\n",
    "            a = unpackAfile(filename + '.A' + sub_fname)\n",
    "            if int(sub_fname) >= 20:\n",
    "                p = unpackPfile_2020(pfile)  # 2020 ~\n",
    "            else:\n",
    "                p = unpackPfile(pfile)   # ~ 2019 \n",
    "            \n",
    "            # 把 afile 資訊加入 pfile's dictionary\n",
    "            p = append_afile_to_pfile(a, p)\n",
    "\n",
    "            # 把 pfile 裡面的 ndarray 轉換成 list 才能存進 dictionary\n",
    "            p = convert_arr_to_list(p)    \n",
    "\n",
    "            # 把沒有加速度資料的測站刪掉\n",
    "            p = delete_no_data(p)\n",
    "\n",
    "            # 改一些欄位\n",
    "            p = copy_values(p)\n",
    "\n",
    "            # 整合一些欄位\n",
    "            p = concat_attributes(p)\n",
    "\n",
    "            # 最後修改 json 欄位\n",
    "            p = modify_json(p, json_file, sub_fname)\n",
    "\n",
    "            # 新制震度\n",
    "            p = modify(p)\n",
    "            \n",
    "            # 清空之前 wave dir 的波型\n",
    "            remove_wave(wave_save_path, p['event'])\n",
    "            \n",
    "            # 波型取出來另外存\n",
    "            p = extractWave(p, wave_save_path, mode)\n",
    "            \n",
    "            # write\n",
    "            with open(save_path, 'w') as file:\n",
    "                json.dump(p, file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('fix: ', e)\n",
    "            #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "37dcb59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [02:53<00:00, 13.36s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.69s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:34<00:00, 17.25s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [06:14<00:00, 41.60s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:22<00:00, 20.71s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [08:58<00:00, 14.17s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:24<00:00, 28.03s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:38<00:00, 39.59s/it]\n"
     ]
    }
   ],
   "source": [
    "#all_year = [str(y) for y in range(2012, 2022)]\n",
    "all_year = [str(y) for y in range(2013, 2021)]\n",
    "\n",
    "for year in all_year:\n",
    "    print('year: ', year)\n",
    "    sub_fname = year[2:]\n",
    "    \n",
    "    mode = 'felt'\n",
    "    #mode = 'nofelt'\n",
    "\n",
    "    base_path = '/mnt/nas6/origin_CWB_data/CWB_felt/' + year + '/felt' \n",
    "    #base_path = '/mnt/nas8/CWBSN/' + year  \n",
    "\n",
    "    save_base_path = os.path.join('/mnt/nas2/CWBSN', year)\n",
    "    #save_base_path = os.path.join('/mnt/nas2/CWBSN_nofelt', year)\n",
    "\n",
    "    wave_save_path = os.path.join(save_base_path, 'wave')\n",
    "    files = get_failed(save_base_path, base_path)\n",
    "    \n",
    "    fix(files, base_path, wave_save_path, sub_fname, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bd0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthquake",
   "language": "python",
   "name": "earthquake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
